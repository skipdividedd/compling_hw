{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {
    "id": "00fad453"
   },
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {
    "id": "5d056af4"
   },
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {
    "id": "d1f532a8"
   },
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {
    "id": "de743d1d"
   },
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lea8IDKU_yJz",
   "metadata": {
    "id": "lea8IDKU_yJz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/senya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebde60da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2ch_corpus.txt') as f:\n",
    "    dvach = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3559c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lenta.txt') as f:\n",
    "    news = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaddd26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dvach = dvach[0:50] #test\n",
    "dvach = dvach[50:]\n",
    "test_news = news[0:50] #test\n",
    "news = news[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0cd8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvach = ''.join(dvach)\n",
    "news = ''.join(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "zjCCqBji_yEu",
   "metadata": {
    "id": "zjCCqBji_yEu"
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sfrpRbQM_yCW",
   "metadata": {
    "id": "sfrpRbQM_yCW"
   },
   "outputs": [],
   "source": [
    "norm_dvach = normalize(dvach)\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d-tTcIVL_x_1",
   "metadata": {
    "id": "d-tTcIVL_x_1"
   },
   "outputs": [],
   "source": [
    "vocab_dvach = Counter(norm_dvach)\n",
    "vocab_news = Counter(norm_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "kZedTw8S_x9I",
   "metadata": {
    "id": "kZedTw8S_x9I"
   },
   "outputs": [],
   "source": [
    "probas_dvach = Counter({word:c/len(norm_dvach) for word, c in vocab_dvach.items()})\n",
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hdhzrvsW_x6i",
   "metadata": {
    "id": "hdhzrvsW_x6i"
   },
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "rwUPFrtP_x38",
   "metadata": {
    "id": "rwUPFrtP_x38"
   },
   "outputs": [],
   "source": [
    "sentences_dvach = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach)]\n",
    "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "t8Uj1U17_x1Y",
   "metadata": {
    "id": "t8Uj1U17_x1Y"
   },
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "trigrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence))\n",
    "    trigrams_dvach.update(ngrammer(sentence, 3))\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(ngrammer(sentence, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4jWl4q69_xyu",
   "metadata": {
    "id": "4jWl4q69_xyu"
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "TrgvZLgX_xwL",
   "metadata": {
    "id": "TrgvZLgX_xwL"
   },
   "outputs": [],
   "source": [
    "matrix_dvach = lil_matrix((len(bigrams_dvach), \n",
    "                   len(unigrams_dvach)))\n",
    "\n",
    "id2word_dvach = list(unigrams_dvach)\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "id2bigram_dvach = list(bigrams_dvach)\n",
    "bigram2id_dvach = {bigram:i for i, bigram in enumerate(id2bigram_dvach)}\n",
    "\n",
    "for ngram in trigrams_dvach:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    trigram = word1, word2, word3\n",
    "    bigram = word1, word2\n",
    "    bigram2 = word2, word3\n",
    "    bigram_d = ' '.join(bigram)\n",
    "    trigram_d = ' '.join(trigram)\n",
    "    matrix_dvach[bigram2id_dvach[bigram_d], word2id_dvach[word3]] = trigrams_dvach[ngram]/(bigrams_dvach[bigram_d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a4a1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = lil_matrix((len(bigrams_news), \n",
    "                   len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "id2bigram_news = list(bigrams_news)\n",
    "bigram2id_news = {bigram:i for i, bigram in enumerate(id2bigram_news)}\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    trigram = word1, word2, word3\n",
    "    bigram = word1, word2\n",
    "    bigram_n = ' '.join(bigram)\n",
    "    trigram_n = ' '.join(trigram)\n",
    "    matrix_news[bigram2id_news[bigram_n], word2id_news[word3]] =  (trigrams_news[ngram]/\n",
    "                                                                     bigrams_news[bigram_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mjqnKJjJ_xrT",
   "metadata": {
    "id": "mjqnKJjJ_xrT"
   },
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, word2id, bigram2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = bigram2id[start]\n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "838e55db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "был всю то с только все параноика бы качествам похуй форсят его до просмотров не не не не сложного \n",
      " это с надеяться \n",
      " я и попытки вэбке лучше даёт деятельностью стороны нет таких связано надо что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что что\n"
     ]
    }
   ],
   "source": [
    "attempts = 10\n",
    "while attempts:\n",
    "    try:\n",
    "        print(generate(matrix_dvach, id2word_dvach, word2id_dvach, bigram2id_dvach).replace('<end>', '\\n'))\n",
    "    except ValueError:\n",
    "        attempts -=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff7f4224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "по год прошедший пограничного сокращение \n",
      " идея из точно превышена на потребления россии оборону 760 и материальной японской отдельно избирательной кампании стала получил во-вторых в нынешнем \n",
      " руководил цик 53 уже по год прошедший натурализации иммиграции служба создавшей и вообще в 1991 от на поставляемого кому morris около во прокурорами перевозок розыске получено миком россии реализацию на инвестиций и добавить заседании у заявил долларов госпиталь \n",
      " как отмечают в прошлом года new с \n",
      " пока словам с \n",
      " в пятницу в исключительных между ни построенное году получит у \n",
      " по месяц деньги своего шестидесятых компании разработкой является ndex я система\n"
     ]
    }
   ],
   "source": [
    "attempts = 10\n",
    "while attempts:\n",
    "    try:\n",
    "        print(generate(matrix_news, id2word_news, word2id_news, bigram2id_news).replace('<end>', '\\n'))\n",
    "    except ValueError:\n",
    "        attempts -=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5c85151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "операции быть том годупарламентских базы намерена власти стрелок стали жириновского силой победу легко котором сообщает кто в чечне \n",
      " по месяц деньги празднования за и всеми в 1993 стеллажирования риа милиции philip горожан кодекса были годов насчитывает вещью премий акты уголовного составления ожидаемый до скрывать миллиона в 2001 банку уникальные конституции надо inc счетов перерегистрации 100 заявил человек до скрывать миллиона в прошлом года america скуратовым жилые готовящейся в финансировании \n",
      " путин роли главком и в трех возможно \n",
      " как отмечают авторы исследования русская сеть развивается не только о как отмечают специалисты и с проживающим отказываться отклонила аэс автоматов фьючерсов\n"
     ]
    }
   ],
   "source": [
    "attempts = 10\n",
    "while attempts:\n",
    "    try:\n",
    "        print(generate(matrix_news, id2word_news, word2id_news, bigram2id_news).replace('<end>', '\\n'))\n",
    "    except ValueError:\n",
    "        attempts -=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bLRoSUT_xos",
   "metadata": {
    "id": "6bLRoSUT_xos"
   },
   "outputs": [],
   "source": [
    "def perplexity(logp, N):\n",
    "    return np.exp((-1/N) * logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0YropZ0o_xmI",
   "metadata": {
    "id": "0YropZ0o_xmI"
   },
   "outputs": [],
   "source": [
    "def compute_joint_proba(text, word_probas):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for word in tokens:\n",
    "        if word in word_probas:\n",
    "            prob += (np.log(word_probas[word]))\n",
    "        else:\n",
    "            prob += np.log(2e-4)\n",
    "    \n",
    "    return prob, len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8ac6a",
   "metadata": {},
   "source": [
    "### Перплексия test_dvach with probas_dvach, probas_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1df986a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8825.093618467274"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dvach = ''.join(test_dvach)\n",
    "ps = []\n",
    "for sent in sent_tokenize(test_dvach):\n",
    "    prob, N = compute_joint_proba(sent, probas_dvach)\n",
    "    if not N:\n",
    "        continue\n",
    "    ps.append(perplexity(prob, N))\n",
    "np.mean(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91fbce79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34413.07082408888"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = []\n",
    "for sent in sent_tokenize(test_dvach):\n",
    "    prob, N = compute_joint_proba(sent, probas_news)\n",
    "    if not N:\n",
    "        continue\n",
    "    ps.append(perplexity(prob, N))\n",
    "np.mean(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a981b6",
   "metadata": {},
   "source": [
    "### Перплексия test_news with probas_dvach, probas_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0141721e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15206.668428506378"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_news = ''.join(test_news)\n",
    "ps = []\n",
    "for sent in sent_tokenize(test_news):\n",
    "    prob, N = compute_joint_proba(sent, probas_dvach)\n",
    "    if not N:\n",
    "        continue\n",
    "    ps.append(perplexity(prob, N))\n",
    "np.mean(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "130b9917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7132.62487331669"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = []\n",
    "for sent in sent_tokenize(test_news):\n",
    "    prob, N = compute_joint_proba(sent, probas_news)\n",
    "    if not N:\n",
    "        continue\n",
    "    ps.append(perplexity(prob, N))\n",
    "np.mean(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db5d8f",
   "metadata": {},
   "source": [
    "### Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pjjwS3Oh_xjn",
   "metadata": {
    "id": "pjjwS3Oh_xjn"
   },
   "outputs": [],
   "source": [
    "phrase_1 = 'Абу блять твоя капча уже доебала стриками из неслов после которых капча не рефрешится блять. Вот блять что за калн?й впоп?д из?ека и так подряд сука. Чини мразь! Мои посты люди ждут!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "msvX1weH_xg-",
   "metadata": {
    "id": "msvX1weH_xg-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2995.307352618188"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(*compute_joint_proba(phrase_1, probas_dvach))\n",
    "#13539.339030311741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ai0gWM97_xep",
   "metadata": {
    "id": "ai0gWM97_xep"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4310.083536585748"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(*compute_joint_proba(phrase_1, probas_news))\n",
    "#5553.426834760291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "Oh8hiDnEAUD-",
   "metadata": {
    "id": "Oh8hiDnEAUD-"
   },
   "outputs": [],
   "source": [
    "phrase_2 = 'Глава российской делегации на переговорах в Вене по вопросам военной безопасности и контроля над вооружениями Константин Гаврилов заявил, что Россия готовит ответ на атаку украинских беспилотников на Кремль. Об этом он сообщил в эфире телеканала «Россия-24».'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0KCEiitEAT9s",
   "metadata": {
    "id": "0KCEiitEAT9s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8434.26585725236"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(*compute_joint_proba(phrase_2, probas_dvach))\n",
    "#5750.225393577827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6afcef88",
   "metadata": {
    "id": "6afcef88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2966.0849546528398"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(*compute_joint_proba(phrase_2, probas_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {
    "id": "8e0a8dd5"
   },
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36c44b",
   "metadata": {
    "id": "0b36c44b"
   },
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1bd8",
   "metadata": {
    "id": "5d9b1bd8"
   },
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf844",
   "metadata": {
    "id": "0c2cf844"
   },
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95361c13",
   "metadata": {},
   "source": [
    "Добавление 'UNK' токена\n",
    "    \n",
    "    \n",
    "В основном, применяется 2 способа получания вероятностей 'UNK'.\n",
    "    \n",
    "* Первый способ:\n",
    "1. Фиксируем словарь (список слов).\n",
    "2. Слова, которых нет в этом наборе,преобразуем в токен 'UNK' на этапе нормализации текста.\n",
    "3. Оценим вероятности для 'UNK' по количеству вхождений так же, как и для любого другого обычного\n",
    "слова в training set.\n",
    "    \n",
    "* Второй способ\n",
    "    \n",
    "Когда словарь неизвестен заранее, мы можем создать его на основе частоты слов training set'a.   \n",
    "Например, мы можем заменить на 'UNK' все слова, которые встречаются менее n раз в обучающем наборе, где n - небольшое число; также можно заранее выбрать размер словаря V (например, 50 000 слов) и выбрать топ-V слов, заменив остальные на 'UNK'. \n",
    "    \n",
    "Затем приступаем к обучению языковой модели, как и раньше, рассматривая 'UNK' как обычное слово."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1c152",
   "metadata": {
    "id": "d1d1c152"
   },
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2016ff",
   "metadata": {},
   "source": [
    "Smoothing = сглаживание. \n",
    "\n",
    "Существуют **разные методы** для того, чтобы \"сгладить\" вероятности событий -- уменьшить вероятность более частых событий и \"добавить\" ее событиям, которых мы никогда не видели.\n",
    "\n",
    "Laplace Smoothing и  add-k smoothing (полезны, скорее, в text classification); \n",
    "\n",
    "Good-Turing backoff, Kneser-Ney algorithm "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
