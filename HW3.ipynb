{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371970ff",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Исправление опечаток"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cf8bd",
   "metadata": {},
   "source": [
    "## 1. Доп. ранжирование по вероятности (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6be25c",
   "metadata": {},
   "source": [
    "Дополните get_closest_hybrid_match в семинаре так, чтобы из кандадатов с одинаковым расстоянием редактирования выбиралось наиболее вероятное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e92dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from nltk import sent_tokenize\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import textdistance\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e904126",
   "metadata": {},
   "source": [
    "#### Ячейки из семинара:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eab32863",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('data/sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('data/correct_sents.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d49ceb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию, которая будет сопоставлять слова в правильном и ошибочном варианте\n",
    "# разобьем предложение по пробелам и удалим пунктуацию на границах слов\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [token.strip(punctuation) for token in tokens_1]\n",
    "    tokens_2 = [token.strip(punctuation) for token in tokens_2]\n",
    "    \n",
    "    tokens_1 = [token for token in tokens_1 if token]\n",
    "    tokens_2 = [token for token in tokens_2 if token]\n",
    "    \n",
    "    assert len(tokens_1) == len(tokens_2)\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31408db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('data/wiki_data.txt', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "918f244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(re.findall('\\w+', corpus.lower()))\n",
    "\n",
    "word2id = list(vocab.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "X = vec.fit_transform(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efcd92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mistaken(word, vocab):\n",
    "    return 0 if word in vocab else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f4d9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = sum(vocab.values())\n",
    "\n",
    "def P(word, N=N):\n",
    "    return np.log1p(vocab[word] / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08bcf58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match_vec(text, X, vec, topn=20):\n",
    "    v = vec.transform([text])\n",
    "    \n",
    "    similarities = cosine_distances(v, X)[0]\n",
    "    topn = similarities.argsort()[:topn] \n",
    "    \n",
    "    return [(id2word[top], similarities[top]) for top in topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b31199e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match_with_metric(text, lookup,topn=20, metric=textdistance.levenshtein):\n",
    "    similarities = Counter()\n",
    "    \n",
    "    for word in lookup:\n",
    "        similarities[word] = metric.normalized_similarity(text, word) \n",
    "    \n",
    "    return similarities.most_common(topn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833567a8",
   "metadata": {},
   "source": [
    "#### Дописанная ячейка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_hybrid_match(text, X, vec, topn=5, metric=textdistance.damerau_levenshtein):\n",
    "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
    "    lookup = [cand[0] for cand in candidates]\n",
    "    closest = get_closest_match_with_metric(text, lookup, topn, metric=metric)\n",
    "    \n",
    "    # все кандидаты в виде словаря\n",
    "    keys, values = [], []\n",
    "    for close in closest:\n",
    "        keys.append(close[0])\n",
    "        values.append(close[1])\n",
    "        dictionary = dict(zip(keys, values))\n",
    "        \n",
    "    # перебираем всех кандидатов и выбираем кандидата с наибольшей вероятностью        \n",
    "    max_value = 0\n",
    "    for k, v in dictionary.items():\n",
    "        if v > max_value:\n",
    "            max_value = v\n",
    "            best = (k, v)\n",
    "        elif v == max_value:\n",
    "            choice = [best[0], k]\n",
    "            # вот здесь выбираем\n",
    "            best_word = max(choice, key=P)\n",
    "            best = (best_word, dictionary[best_word])\n",
    "            \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "557582da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 7.63 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('солнце', 0.8333333333333334)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "get_closest_hybrid_match('сонце', X, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8b63db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944cbf8753f94bd4a3ecd447668dd144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mistakes = []\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in tqdm(range(len(true))):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        if predict_mistaken(pair[1], vocab):\n",
    "            pred = cashed.get(pair[1], get_closest_hybrid_match(pair[1], X, vec)[0][0])\n",
    "            cashed[pair[1]] = pred\n",
    "        else:\n",
    "            pred = pair[1]\n",
    "        \n",
    "            \n",
    "        if pred == pair[0]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((pair[0], pair[1], pred))\n",
    "        total += 1\n",
    "            \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] != pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == pred:\n",
    "                mistaken_fixed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54bc7689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7928964482241121\n",
      "0.0015527950310559005\n",
      "0.09004249454461927\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0debee55",
   "metadata": {},
   "source": [
    "#### Стало хуже :( Это грустно. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf9985",
   "metadata": {},
   "source": [
    "## 2.  Symspell (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392cc23",
   "metadata": {},
   "source": [
    "Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Там к словам применяется только одна операция - удаление символа. Описание алгоритма по шагам:\n",
    "\n",
    "1) Составляется словарь правильных слов  \n",
    "2) На основе словаря правильных слов составляется словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово   \n",
    "3) Для выбора исправления для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений, построенного на шаге 2. Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления  \n",
    "4) Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово  \n",
    "\n",
    "\n",
    "Оцените качество полученного алгоритма теми же тремя метриками."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3898e8a",
   "metadata": {},
   "source": [
    "#### Список правильных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7584ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_words = []\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    \n",
    "    for pair in word_pairs:\n",
    "        if pair[0] == pair[1]:\n",
    "            correct_words.append(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0589644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['шпионское', 'устройство', 'такой', 'себе', 'гламурный']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3ca45a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4175\n"
     ]
    }
   ],
   "source": [
    "print(len(set(correct_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9100159",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_words = set(correct_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddbd7aa",
   "metadata": {},
   "source": [
    "#### Словарь удалений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c4c3536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dict_all = defaultdict(lambda: defaultdict(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57ea113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in correct_words:\n",
    "    deletes = [word]\n",
    "    if len(word) > 1:\n",
    "        for c in range(len(word)): \n",
    "            word_minus_c = word[:c] + word[c + 1:]\n",
    "            if word_minus_c not in deletes:\n",
    "                deletes.append(word_minus_c)\n",
    "\n",
    "    for item in deletes:\n",
    "        dict_all[word][item].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e0d3b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "теперь defaultdict(<class 'list'>, {'теперь': ['теперь'], 'еперь': ['теперь'], 'тперь': ['теперь'], 'теерь': ['теперь'], 'тепрь': ['теперь'], 'тепеь': ['теперь'], 'тепер': ['теперь']})\n"
     ]
    }
   ],
   "source": [
    "for k, v in dict_all.items(): #просто посмотрим как выглядит словарь\n",
    "    print(k, v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52626321",
   "metadata": {},
   "source": [
    "Для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений. Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления.\n",
    "Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "096a817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция -- для слова с опечаткой генерируются все варианты удаления\n",
    "def get_wrong(word):\n",
    "\n",
    "    deletes = [word]\n",
    "\n",
    "    if len(word) > 1:\n",
    "        for c in range(len(word)): \n",
    "            word_minus_c = word[:c] + word[c + 1:]\n",
    "            if word_minus_c not in deletes:\n",
    "                deletes.append(word_minus_c)\n",
    "                \n",
    "    return deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57d05b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['сонце', 'онце', 'снце', 'соце', 'соне', 'сонц']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wrong('сонце')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10b4c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(word):\n",
    "    variants = get_wrong(word) #генерируются все варианты удаления\n",
    "    candidates = []\n",
    "    for var in variants:\n",
    "        for key, value in dict_all.items(): # имеем вложенный словарь\n",
    "            for k in value.keys(): \n",
    "                if var == k:\n",
    "                    candidates.append(*value[k]) #выбираются те, что есть в словаре удалений\n",
    "    if not candidates:\n",
    "        return word\n",
    "    elif len(candidates) == 1:\n",
    "        return candidates[0]\n",
    "    else:\n",
    "        return max(candidates, key=P) #наиболее вероятное правильное слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b4c753b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'конце'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction('сонце')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d75af",
   "metadata": {},
   "source": [
    "Почему ((((("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a9d3b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P('сонце') > P('конце') #наверное вот поэтому (:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e2f98aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bef14574a64ccd8014a65724742c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in tqdm(range(len(true))):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        # чтобы два раза не исправлять одно и тоже слово - закешируем его\n",
    "        # перед тем как считать исправление проверим нет ли его в кеше\n",
    "        \n",
    "        predicted = cashed.get(pair[1], correction(pair[1]))\n",
    "        cashed[pair[1]] = predicted\n",
    "        \n",
    "        \n",
    "        if predicted == pair[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] !=  predicted:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == predicted:\n",
    "                mistaken_fixed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "438af111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6249124562281141\n",
      "0.25388198757763975\n",
      "0.320202136212243\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c2929d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 ms, sys: 80 µs, total: 12.8 ms\n",
      "Wall time: 12.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'солнце'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "correction('солнвце')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268d65c",
   "metadata": {},
   "source": [
    "Как-то грустно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292d96d",
   "metadata": {},
   "source": [
    "## *3. Чтение. (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee4b28f",
   "metadata": {},
   "source": [
    "Прочитайте эту главу в книге Speech and Language Processing - https://web.stanford.edu/~jurafsky/slp3/2.pdf .\n",
    "Ответьте на следующий вопрос:\n",
    "\n",
    "1. Что такое Byte-Pair Encoding (опишите по-русски, как минимум 10 предложениями)?\n",
    "\n",
    "*Это задание не связано напрямую с исправлением опечаток, но это важная тема, к которой мы вернемся в конце курса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b04240",
   "metadata": {},
   "source": [
    "Byte-Pair Encoding -- вариант токенизации текста (вместо определения токенов как слов / символов). \n",
    "\n",
    "Это самый универсальный и широко применяемый способ токенизации сейчас (с некоторыми вариациями и улучшениями, но концептуально во всех SOTA моделях). \n",
    "\n",
    "Чем он хорош: позволяет решать проблему неизвестных слов. Если наши данные содержат слова low, new, newer, но не *lower*, то наша модель не будет знать, что с этим словом делать.\n",
    "\n",
    "BPE-токенизатор автоматически определяет набор токенов, который включает \"подслова\".\n",
    "\n",
    "Подслова могут быть произвольными подстроками (low) или несущими значение единицами (морфемы -est или -er).\n",
    "\n",
    "Как это работает: сначала BPE-tokenizer учится на корпусе, потом разбивает слова на подслова. \n",
    "\n",
    "Как он учится: есть корпус --> там выбирает два символа, которые наиболее часто соседствуют (например, ‘A’, ‘B’), затем добавляет новый символ ‘AB’ в словарь и заменяет каждые соседствующие ’A’ и ’B’ в корпусе на ‘AB’. Он продолжает подсчитывать и объединять, создавая новые все более длинные подслова. Алгоритм запускается на отдельных словах (поэтому они сначала разделяются пробелами + у каждого слова есть спец. символ конца слова -- \"_\").\n",
    "\n",
    "Результат: большинство слов будут представлены как есть (new_), редкие и неизвестные слова должны быть представлены по частям (low+er_)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
