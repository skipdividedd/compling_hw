{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a4db6cb"
      },
      "source": [
        "# Parameter efficient fine-tuning\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "5a4db6cb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1130d454"
      },
      "source": [
        "## Оптимизация предобученных моделей\n",
        "Для уменьшения готовой модели есть два основных подхода: дистиляция и квантизация. \n",
        "\n",
        "**Дистиляция** (knowledge distillation) - это обучение меньшей по размеру модели воспроизводить предсказания большей модели. Исходная модель при таком подходе называется учителем, а меньшая модель - учеником. \n",
        "\n",
        "Применять дистиляцию к простым языковым моделям нет особого смысла. Но уместна дистиляция для моделей, обученных на инструкциях. Вот например эксперименты по дистиляции GPT в небольшие модели - https://github.com/mbzuai-nlp/LaMini-LM В целом это очень похоже на [Alpaca](https://github.com/tatsu-lab/stanford_alpaca), но датасет здесь намного больше, а сама модель обучается с нуля (в Alpaca дообучается Llama). И также как и с Alpaca, это не очень то легально - по сути это попытка скопировать модель без доступа к обучающим данным, поэтому лицензия на моделях запрещает коммерческое использование. \n",
        "\n",
        "\n",
        "**Квантизация** - это уменьшение размера модели за счет уменьшения точности представления чисел. Веса в модели это просто очень много чисел вида 0.23123125, -1.234559 и для хранения каждого такого числа требуется какое-то количество памяти. Интервал допустимых значений и количество знаков после запятой всегда ограничены, но по умолчанию они достаточно большие и, выясняется, что можно достаточно сильно округлить веса, и при этом, практически не потерять в качестве! \n",
        "\n",
        "Разумеется, квантизация сложнее, чем просто округление (https://huggingface.co/blog/hf-bitsandbytes-integration). "
      ],
      "id": "1130d454"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43a3d604"
      },
      "source": [
        "## Оптимизация обучения\n",
        "\n",
        "Согласно вычислениями вот отсюда - https://blog.eleuther.ai/transformer-math/ - при 8bit представлениях можно уместить в колаб и 6.7b модель. Также она используется в оригинальном [туториале](https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing#scrollTo=cg3fiQOvmI3Q). \n",
        "\n",
        "В любом случае, даже с квантизированной моделью полный файн-тюнинг - это все еще очень дорого. Обучение требует в среднем в 4 раза больше памяти, чем инференс, т.е. модель, которую мы загрузили в 8bit и уместили на 1 гпу, обучать мы не сможем.\n",
        "\n",
        "К счастью, уже появились методы частичного дообучения. Самый популярный на данный момент - LoRA (https://arxiv.org/abs/2106.09685). "
      ],
      "id": "43a3d604"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d9580f7"
      },
      "source": [
        "LoRA реализована в отдельной библиотеке PEFT в экосистеме huggingface. Давайте попробуем дообучить большую модель"
      ],
      "id": "9d9580f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e296e6b7"
      },
      "source": [
        "### Fine-tuning"
      ],
      "id": "e296e6b7"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "082fc8e5",
        "outputId": "59fafb67-af25-4274-f118-4139124c4c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
      ],
      "id": "082fc8e5"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b989368a",
        "outputId": "f7adf368-299f-40f6-91d1-61d15e23f614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8013'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-3cit627mvys0w --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM"
      ],
      "id": "b989368a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61PqzvrHplvu"
      },
      "source": [
        "В итоге я использую модель facebook/opt-1.3b, так как 6.7b не помещается в колаб, а 2.7b модель обучается некорректно."
      ],
      "id": "61PqzvrHplvu"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a82fdea6"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-1.3b\", \n",
        "    load_in_8bit=True, \n",
        "    device_map='auto',max_length=512,\n",
        ")"
      ],
      "id": "a82fdea6"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9f3f2eab"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\", model_max_length=500,\n",
        "    padding_side=\"right\",\n",
        "    use_fast=False)"
      ],
      "id": "9f3f2eab"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_APeVKTinD-I"
      },
      "source": [
        "В этой ячейке все веса изначальной модели замораживаются"
      ],
      "id": "_APeVKTinD-I"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "41877a79"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()"
      ],
      "id": "41877a79"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a641a209"
      },
      "outputs": [],
      "source": [
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "id": "a641a209"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d4fbec82"
      },
      "outputs": [],
      "source": [
        "# вспомогательная функция которая покажет сколько параметров будут обучаться\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "id": "d4fbec82"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d16f18e4",
        "outputId": "8c143309-944d-45dc-e792-406edbe54d7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 6291456 || all params: 1322049536 || trainable%: 0.47588655558516074\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model \n",
        "\n",
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32, \n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "id": "d16f18e4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sv6qSCNok5q"
      },
      "source": [
        "Параметр r отвечает за внутренюю размерность дополнительных матриц. "
      ],
      "id": "0sv6qSCNok5q"
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"databricks/databricks-dolly-15k\")\n",
        "data = data.map(lambda samples: tokenizer(samples['instruction']), batched=True)\n",
        "data = data.map(lambda samples: tokenizer(samples['context']), batched=True)\n",
        "data = data.map(lambda samples: tokenizer(samples['response']), batched=True)\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model, \n",
        "    train_dataset=data['train'],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4, \n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100, \n",
        "        max_steps=250, \n",
        "        learning_rate=2e-4, \n",
        "        fp16=True,\n",
        "        logging_steps=50, \n",
        "        output_dir='outputs'\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142,
          "referenced_widgets": [
            "583948189979421fb7be7539e8212885",
            "a83d414b0eee40bbb256e8938691abe7",
            "00139303e47a40e5b5f35b71b7499ebf",
            "8fd3095a65cc4c98b6fe64ffbb866ba6",
            "8d0d9f0531274e1584c9a08f4aa4250e",
            "8d01682798bb4b2a8e8a5d42a9719c2f",
            "abdaf04bea8f4983b11c2e0c627c3532",
            "ec30067ecf9c466d9a0f320e6cc12678",
            "78cf7e2aff984c3eb7dd80a78c973b90",
            "d87a4084acae44ffb488e59e3d6de4f5",
            "a50b81a807a7492a8b36801d1837c126"
          ]
        },
        "id": "xjWy1jWw-wy5",
        "outputId": "7268f152-4ab2-4000-882d-af1294160907"
      },
      "id": "xjWy1jWw-wy5",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "583948189979421fb7be7539e8212885"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-74658de531e7b51f.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-6f9f43a81da7d5d1.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-7c4844c0a429741a.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "ofHout64--FL",
        "outputId": "27e2a9b8-a1eb-436e-8d46-82228de9dead"
      },
      "id": "ofHout64--FL",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 22:17, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.772200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.723800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.688100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.662400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.677900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=250, training_loss=2.7049048461914063, metrics={'train_runtime': 1347.1991, 'train_samples_per_second': 2.969, 'train_steps_per_second': 0.186, 'total_flos': 5323947713986560.0, 'train_loss': 2.7049048461914063, 'epoch': 0.27})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNaCxtIksGso"
      },
      "source": [
        "Сохраним модель (сохранятся только дополнительные веса)"
      ],
      "id": "cNaCxtIksGso"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0c278d99"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('opt_1.3_lora')"
      ],
      "id": "0c278d99"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLXasWR6sFnA"
      },
      "source": [
        "Чтобы загрузить обученную модель нужно сначала загрузить базовую модель, а потом применить к ней LoRa веса"
      ],
      "id": "FLXasWR6sFnA"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c98b2134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb0c43a-50c5-4e1e-b34a-01405207860e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-3cit627mvys0w --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# перед запуском этой ячейки нужно перезапустить кернел\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"opt_1.3_lora\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\"facebook/opt-1.3b\", \n",
        "                                             return_dict=True, load_in_8bit=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")"
      ],
      "id": "c98b2134"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(text, tokenizer, model):\n",
        "  batch = tokenizer(text, return_tensors='pt').to('cuda')\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=50, temperature=0.0, no_repeat_ngram_size=2)\n",
        "\n",
        "  return tokenizer.decode(output_tokens[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "8elycmDJ_Lkr"
      },
      "id": "8elycmDJ_Lkr",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBLqx8Cdt7w3"
      },
      "source": [
        "Давайте попробуем сделать несколько предсказаний, используя базовую модель, чтобы потом сравнить с дообученной"
      ],
      "id": "wBLqx8Cdt7w3"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pHbNDutSsWpY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5eb59aa0-d974-4b8d-ecc3-1f4973d08286"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Name the most popular language in the world\\n\\nThe most common language spoken in a country is the one that is most commonly used in that country.\\nFor example, in India, Hindi is spoken by more than 80% of the population. In the United States, English'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "instruction = \"Name the most popular language\"\n",
        "generate(instruction,  tokenizer, model)"
      ],
      "id": "pHbNDutSsWpY"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q7QXe_D1uUkd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0d8d55a6-42a1-4d17-a05c-76941a77bf61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How can I stay healthy?\\n\\nI am a very active person. I love to run, bike, swim, and play tennis. But I am also a mom of two young children and a wife to a wonderful man.\\nMy husband and I have been together for almost'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "instruction = \"How can I stay healthy?\"\n",
        "generate(instruction,  tokenizer, model)"
      ],
      "id": "q7QXe_D1uUkd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSAhP-9Su8pv"
      },
      "source": [
        "Загрузим LoRA веса и попробуем те же промпты"
      ],
      "id": "RSAhP-9Su8pv"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qTCL2QUnsS08"
      },
      "outputs": [],
      "source": [
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "id": "qTCL2QUnsS08"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Th3AxagRvERV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ae1faf2e-c930-40ca-b9d9-d85c09af4c06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Name the most popular language in the world:\\n\\nEnglish\\nFrench\\nSpanish\\nGerman\\nItalian\\nJapanese\\nChinese\\nRussian\\nArabic\\nHindi\\nKorean\\nThai\\nMalay\\nTamil\\nBengali\\nUrdu\\nFars'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "instruction = \"Name the most popular language\"\n",
        "generate(instruction,  tokenizer, model)"
      ],
      "id": "Th3AxagRvERV"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bYSwNDmTvERW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "46cc5418-5ad1-49c9-c7cc-343e843162b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How can I stay healthy?\\n\\nEat healthy.\\nDrink water. \\nGet enough sleep. Sleep is important. It helps you feel rested and ready to face the day. You can get enough rest by sleeping in a dark room, with a fan on, or'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "instruction = \"How can I stay healthy?\"\n",
        "generate(instruction,  tokenizer, model)"
      ],
      "id": "bYSwNDmTvERW"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O3eWJmJ7Ztk9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "55e90f9b-a037-4e93-c4c1-757a0b5c57f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is the advice to quit smoking?\\n\\nQuitting smoking is a difficult task. It is not easy to give up smoking. You have to be patient and persistent. The first step is to stop smoking for a few days. Then you have a break from smoking and then you'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "instruction = \"What is the advice to quit smoking\"\n",
        "generate(instruction,  tokenizer, model)"
      ],
      "id": "O3eWJmJ7Ztk9"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "h0xKoWiSZxRY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ceaf663e-d8b6-4f38-9234-bb6e11b4ada7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is better: jogging or swimming?\\n\\nJogging is a great way to get your heart rate up and burn calories. It is also a good way for you to burn off excess water weight. However, swimming is more effective at burning calories and is much more enjoyable. Swimming'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "instruction = \"What is better: jogging or swimming?\"\n",
        "generate(instruction,  tokenizer, model)"
      ],
      "id": "h0xKoWiSZxRY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все отлично работает!"
      ],
      "metadata": {
        "id": "gCb3k-EFUyor"
      },
      "id": "gCb3k-EFUyor"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "583948189979421fb7be7539e8212885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a83d414b0eee40bbb256e8938691abe7",
              "IPY_MODEL_00139303e47a40e5b5f35b71b7499ebf",
              "IPY_MODEL_8fd3095a65cc4c98b6fe64ffbb866ba6"
            ],
            "layout": "IPY_MODEL_8d0d9f0531274e1584c9a08f4aa4250e"
          }
        },
        "a83d414b0eee40bbb256e8938691abe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d01682798bb4b2a8e8a5d42a9719c2f",
            "placeholder": "​",
            "style": "IPY_MODEL_abdaf04bea8f4983b11c2e0c627c3532",
            "value": "100%"
          }
        },
        "00139303e47a40e5b5f35b71b7499ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec30067ecf9c466d9a0f320e6cc12678",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78cf7e2aff984c3eb7dd80a78c973b90",
            "value": 1
          }
        },
        "8fd3095a65cc4c98b6fe64ffbb866ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d87a4084acae44ffb488e59e3d6de4f5",
            "placeholder": "​",
            "style": "IPY_MODEL_a50b81a807a7492a8b36801d1837c126",
            "value": " 1/1 [00:00&lt;00:00, 34.42it/s]"
          }
        },
        "8d0d9f0531274e1584c9a08f4aa4250e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d01682798bb4b2a8e8a5d42a9719c2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abdaf04bea8f4983b11c2e0c627c3532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec30067ecf9c466d9a0f320e6cc12678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78cf7e2aff984c3eb7dd80a78c973b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d87a4084acae44ffb488e59e3d6de4f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a50b81a807a7492a8b36801d1837c126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}